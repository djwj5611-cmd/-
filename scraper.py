# scraper.py
import os
import time
import pickle
import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# ===== 1. ê³µí†µ ë“œë¼ì´ë²„ ìƒì„± (GitHub Actions í˜¸í™˜) =====
def create_stealth_driver():
    options = Options()
    options.add_argument('--headless=new')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('--disable-gpu')
    options.add_argument('--window-size=1920,1080')
    options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
    
    # ìŠ¤í…”ìŠ¤ ì˜µì…˜ ì¶”ê°€
    options.add_argument('--disable-blink-features=AutomationControlled')
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option('useAutomationExtension', False)
    
    # GitHub Actions í™˜ê²½ì—ì„œëŠ” ChromeDriver ê²½ë¡œë¥¼ ì§ì ‘ ì§€ì •í•  í•„ìš” ì—†ì´
    # setup-chrome ì•¡ì…˜ì´ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•´ì¤ë‹ˆë‹¤.
    try:
        driver = webdriver.Chrome(options=options)
        driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        print("âœ… ìŠ¤í…”ìŠ¤ ë“œë¼ì´ë²„ ìƒì„± ì„±ê³µ")
        return driver
    except Exception as e:
        print(f"âŒ ë“œë¼ì´ë²„ ìƒì„± ì‹¤íŒ¨: {e}")
        return None

# ===== 2. ë°ì´í„° ìˆ˜ì§‘ í•¨ìˆ˜ë“¤ =====

# NAVER (requests)
def collect_naver_trends():
    try:
        print("\n[NAVER] ì‹¤ì‹œê°„ í‚¤ì›Œë“œ ìˆ˜ì§‘ ì‹œì‘...")
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        res = requests.get("https://signal.bz/", headers=headers, timeout=10) # ë©”ì¸ í˜ì´ì§€ ì‚¬ìš©
        soup = BeautifulSoup(res.text, "html.parser")
        # ì‚¬ì´íŠ¸ ë‚´ ì—¬ëŸ¬ ë­í‚¹ í‚¤ì›Œë“œë¥¼ ëª¨ë‘ ëŒ€ìƒìœ¼ë¡œ í•˜ì—¬ ë” ì•ˆì •ì ìœ¼ë¡œ ìˆ˜ì§‘
        keywords = [el.get_text(strip=True) for el in soup.select('.rank-keyword .keyword')]
        unique_keywords = list(dict.fromkeys(keywords)) # ì¤‘ë³µ ì œê±°
        print(f"âœ… [ì„±ê³µ] {len(unique_keywords)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        return unique_keywords[:10]
    except Exception as e:
        print(f"âŒ [ì‹¤íŒ¨] NAVER ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return []

# Google Trends (selenium)
def collect_google_trends(driver):
    try:
        print("\n[GOOGLE] ì¸ê¸° ê²€ìƒ‰ì–´ ìˆ˜ì§‘ ì‹œì‘...")
        driver.get("https://trends.google.com/trends/trendingsearches/daily?geo=KR")
        # ë¦¬ìŠ¤íŠ¸ ì „ì²´ë¥¼ ê°ì‹¸ëŠ” ì»¨í…Œì´ë„ˆê°€ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°
        WebDriverWait(driver, 20).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "div.feed-list-wrapper"))
        )
        elements = driver.find_elements(By.CSS_SELECTOR, "div.feed-item-header div.title a")
        keywords = [el.text.strip() for el in elements if el.text.strip()]
        print(f"âœ… [ì„±ê³µ] {len(keywords)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        return keywords[:10]
    except Exception as e:
        print(f"âŒ [ì‹¤íŒ¨] GOOGLE ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        driver.save_screenshot("google_error.png")
        return []

# DCInside (requests)
def collect_dcinside_trends():
    try:
        print("\n[DCINSIDE] ì‹¤ì‹œê°„ ë² ìŠ¤íŠ¸ ìˆ˜ì§‘ ì‹œì‘...")
        headers = {'User-Agent': 'Mozilla/5.0'}
        url = "https://gall.dcinside.com/board/lists/?id=hit"
        res = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(res.text, "html.parser")
        # ê³µì§€ ì œì™¸í•˜ê³  ê²Œì‹œë¬¼ë§Œ ì„ íƒ
        titles = [el.text.strip() for el in soup.select("tr.ub-content.us-post .gall_tit a:not(.icon_notice)")]
        print(f"âœ… [ì„±ê³µ] {len(titles)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        return titles[:10]
    except Exception as e:
        print(f"âŒ [ì‹¤íŒ¨] DCINSIDE ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return []

# Theqoo (requests)
def collect_theqoo_trends():
    try:
        print("\n[THEQOO] HOT ê²Œì‹œë¬¼ ìˆ˜ì§‘ ì‹œì‘...")
        headers = {'User-Agent': 'Mozilla/5.0'}
        url = "https://theqoo.net/hot"
        res = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(res.text, "html.parser")
        # í´ë˜ìŠ¤ ì´ë¦„ì— ì˜ì¡´í•˜ì§€ ì•Šê³ , ì œëª© ì…€ ë‚´ë¶€ì˜ ë§í¬ë¥¼ ì§ì ‘ ì„ íƒ
        titles = [el.text.strip() for el in soup.select('td.title a') if '[ê³µì§€]' not in el.text]
        print(f"âœ… [ì„±ê³µ] {len(titles)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        return titles[:20] # ë”ì¿ ëŠ” HOT ê²Œì‹œë¬¼ì´ ë§ìœ¼ë¯€ë¡œ 20ê°œ ìˆ˜ì§‘
    except Exception as e:
        print(f"âŒ [ì‹¤íŒ¨] THEQOO ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return []


# Twitter (ì¿ í‚¤ ë¡œê·¸ì¸ ë° ë°ì´í„° ìˆ˜ì§‘)
def twitter_cookie_login(driver):
    try:
        print("\n[TWITTER] ì¿ í‚¤ ë¡œê·¸ì¸ ì‹œë„...")
        driver.get("https://x.com/")
        cookie_file = "twitter_cookies.pkl"
        if not os.path.exists(cookie_file):
            print("âŒ twitter_cookies.pkl íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return False

        with open(cookie_file, "rb") as f:
            cookies = pickle.load(f)
        for cookie in cookies:
            driver.add_cookie(cookie)

        driver.refresh()
        WebDriverWait(driver, 20).until(
            EC.presence_of_element_located((By.XPATH, "//a[@data-testid='AppTabBar_Home_Link']"))
        )
        print("âœ… íŠ¸ìœ„í„° ì¿ í‚¤ ë¡œê·¸ì¸ ì„±ê³µ")
        return True
    except Exception as e:
        print(f"âŒ íŠ¸ìœ„í„° ì¿ í‚¤ ë¡œê·¸ì¸ ì‹¤íŒ¨: {e}")
        driver.save_screenshot("twitter_login_failed.png")
        return False

def collect_twitter_data(driver):
    try:
        print("[TWITTER] íŠ¸ë Œë“œ ìˆ˜ì§‘ ì‹œì‘...")
        driver.get("https://x.com/explore/tabs/trending")
        # íŠ¸ë Œë“œ ë°ì´í„°ê°€ ë¡œë“œë  ë•Œê¹Œì§€ ëª…ì‹œì ìœ¼ë¡œ ëŒ€ê¸°
        WebDriverWait(driver, 20).until(
            EC.presence_of_element_located((By.XPATH, "//div[@data-testid='trend']"))
        )
        trends = driver.find_elements(By.XPATH, "//div[@data-testid='trend']//span")
        # '#'ì´ í¬í•¨ëœ í‚¤ì›Œë“œë§Œ í•„í„°ë§í•˜ì§€ ì•Šê³ , ì£¼ìš” í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜´
        keywords = [t.text for t in trends if t.text.strip() and not t.text.isdigit() and 'Trending' not in t.text and 'posts' not in t.text]
        # ì¤‘ë³µ ì œê±° ë° ìˆœì„œ ìœ ì§€
        unique_keywords = list(dict.fromkeys(keywords))
        print(f"âœ… [ì„±ê³µ] {len(unique_keywords)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        return unique_keywords[:10]
    except Exception as e:
        print(f"âŒ [ì‹¤íŒ¨] TWITTER ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return []

# ===== 3. ë©”ì¸ ì‹¤í–‰ ë¡œì§ =====
def main():
    # Seleniumì„ ì‚¬ìš©í•˜ëŠ” ìˆ˜ì§‘ ì‘ì—…
    driver = create_stealth_driver()
    if driver:
        google_keywords = collect_google_trends(driver)
        
        twitter_keywords = []
        if twitter_cookie_login(driver):
            twitter_keywords = collect_twitter_data(driver)
        driver.quit()
    else:
        google_keywords, twitter_keywords = [], []

    # requestsë¥¼ ì‚¬ìš©í•˜ëŠ” ìˆ˜ì§‘ ì‘ì—… (ë“œë¼ì´ë²„ í•„ìš” ì—†ìŒ)
    naver_keywords = collect_naver_trends()
    dcinside_keywords = collect_dcinside_trends()
    theqoo_keywords = collect_theqoo_trends()

    # ë¦¬í¬íŠ¸ ìƒì„±
    os.makedirs("reports", exist_ok=True)
    report_path = f"reports/Trend_Report_{time.strftime('%Y-%m-%d_%H%M%S')}.txt"
    with open(report_path, "w", encoding="utf-8") as f:
        f.write(f"=== Trend Report ({time.strftime('%Y-%m-%d %H:%M:%S')}) ===\n\n")
        f.write("â–  NAVER\n" + ("\n".join(f"- {k}" for k in naver_keywords) if naver_keywords else "ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨") + "\n\n")
        f.write("â–  Google Trends\n" + ("\n".join(f"- {k}" for k in google_keywords) if google_keywords else "ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨") + "\n\n")
        f.write("â–  DCInside\n" + ("\n".join(f"- {k}" for k in dcinside_keywords) if dcinside_keywords else "ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨") + "\n\n")
        f.write("â–  Theqoo\n" + ("\n".join(f"- {k}" for k in theqoo_keywords) if theqoo_keywords else "ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨") + "\n\n")
        f.write("â–  Twitter(X)\n" + ("\n".join(f"- {k}" for k in twitter_keywords) if twitter_keywords else "ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨ ë˜ëŠ” ì¿ í‚¤ íŒŒì¼ ì—†ìŒ") + "\n")
    
    print(f"\nğŸ“Š ë¦¬í¬íŠ¸ ì €ì¥ ì™„ë£Œ: {report_path}")

if __name__ == "__main__":
    main()