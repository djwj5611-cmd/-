# scraper.py
import os
import time
import pickle
import requests
import re
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from datetime import datetime, timedelta

# ===== 1. ê³µí†µ ë“œë¼ì´ë²„ ìƒì„± (GitHub Actions í˜¸í™˜) =====
def create_stealth_driver():
    options = Options()
    options.add_argument('--headless=new')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('--disable-gpu')
    options.add_argument('--window-size=1920,1080')
    options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
    options.add_argument('--disable-blink-features=AutomationControlled')
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option('useAutomationExtension', False)
    try:
        driver = webdriver.Chrome(options=options)
        driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        print("âœ… ìŠ¤í…”ìŠ¤ ë“œë¼ì´ë²„ ìƒì„± ì„±ê³µ")
        return driver
    except Exception as e:
        print(f"âŒ ë“œë¼ì´ë²„ ìƒì„± ì‹¤íŒ¨: {e}")
        return None

# ===== 2. ë°ì´í„° ìˆ˜ì§‘ í•¨ìˆ˜ë“¤ (ê°œì„  ë²„ì „) =====
def collect_naver_trends(driver):
    try:
        print("\n[NAVER] ì‹¤ì‹œê°„ í‚¤ì›Œë“œ ìˆ˜ì§‘ ì‹œì‘...")
        driver.get("https://signal.bz/")
        WebDriverWait(driver, 20).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "div.rank-keyword"))
        )
        elements = driver.find_elements(By.CSS_SELECTOR, 'div.rank-keyword span.keyword')
        keywords = [el.get_text(strip=True) for el in elements if el.get_text(strip=True)]
        unique_keywords = list(dict.fromkeys(keywords))
        print(f"âœ… [ì„±ê³µ] {len(unique_keywords)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        return unique_keywords[:10]
    except Exception as e:
        print(f"âŒ [ì‹¤íŒ¨] NAVER ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        driver.save_screenshot("naver_error.png")
        return []

def collect_google_trends(driver):
    try:
        print("\n[GOOGLE] ì¸ê¸° ê²€ìƒ‰ì–´ ìˆ˜ì§‘ ì‹œì‘...")
        driver.get("https://trends.google.com/trends/trendingsearches/daily?geo=KR")
        WebDriverWait(driver, 20).until(
            EC.presence_of_all_elements_located((By.CSS_SELECTOR, "div.feed-item-header div.title a"))
        )
        elements = driver.find_elements(By.CSS_SELECTOR, "div.feed-item-header div.title a")
        keywords = [el.text.strip() for el in elements if el.text.strip()]
        print(f"âœ… [ì„±ê³µ] {len(keywords)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        return keywords[:10]
    except Exception as e:
        print(f"âŒ [ì‹¤íŒ¨] GOOGLE ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        driver.save_screenshot("google_error.png")
        return []

def collect_dcinside_trends():
    try:
        print("\n[DCINSIDE] ì‹¤ì‹œê°„ ë² ìŠ¤íŠ¸ ìˆ˜ì§‘ ì‹œì‘...")
        headers = {'User-Agent': 'Mozilla/5.0'}
        url = "https://gall.dcinside.com/board/lists/?id=hit"
        res = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(res.text, "html.parser")
        titles = []
        for el in soup.select("tr.ub-content.us-post .gall_tit a:not(.icon_notice)"):
            title = el.text.strip()
            cleaned_title = re.sub(r'\s*\[\d+\]$', '', title)
            titles.append(cleaned_title)
        print(f"âœ… [ì„±ê³µ] {len(titles)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        return titles[:10]
    except Exception as e:
        print(f"âŒ [ì‹¤íŒ¨] DCINSIDE ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return []

def collect_theqoo_trends():
    try:
        print("\n[THEQOO] HOT ê²Œì‹œë¬¼ ìˆ˜ì§‘ ì‹œì‘...")
        headers = {'User-Agent': 'Mozilla/5.0'}
        url = "https://theqoo.net/hot"
        res = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(res.text, "html.parser")
        titles = []
        for row in soup.select("table.theqoo_hot_table tr:not(.notice)"):
            title_element = row.select_one('td.title a')
            if title_element:
                titles.append(title_element.text.strip())
        print(f"âœ… [ì„±ê³µ] {len(titles)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        return titles[:20]
    except Exception as e:
        print(f"âŒ [ì‹¤íŒ¨] THEQOO ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return []

def twitter_cookie_login(driver):
    try:
        print("\n[TWITTER] ì¿ í‚¤ ë¡œê·¸ì¸ ì‹œë„...")
        driver.get("https://x.com/")
        cookie_file = "twitter_cookies.pkl"
        if not os.path.exists(cookie_file):
            print("âŒ twitter_cookies.pkl íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return False
        with open(cookie_file, "rb") as f:
            cookies = pickle.load(f)
        for cookie in cookies:
            driver.add_cookie(cookie)
        driver.refresh()
        WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.XPATH, "//a[@data-testid='AppTabBar_Home_Link']")))
        print("âœ… íŠ¸ìœ„í„° ì¿ í‚¤ ë¡œê·¸ì¸ ì„±ê³µ")
        return True
    except Exception as e:
        print(f"âŒ íŠ¸ìœ„í„° ì¿ í‚¤ ë¡œê·¸ì¸ ì‹¤íŒ¨: {e}")
        driver.save_screenshot("twitter_login_failed.png")
        return False

def collect_twitter_data(driver):
    print("\n[TWITTER] ì±„ìš© ê³µê³  ìƒì„¸ ê²€ìƒ‰ ì‹œì‘...")
    base_keywords = ["ìŠ¤í…", "ìŠ¤íƒœí”„", "ì§ì›", "íŒ€ì›", "íŒ€", "ë§¤ë‹ˆì €"]
    action_keywords = ["ê³µê³ ", "êµ¬ì¸", "ì±„ìš©", "ëª¨ì§‘"]
    search_keywords = [f"{base} {action}" for base in base_keywords for action in action_keywords]
    
    all_results = []
    processed_links = set()

    for keyword in search_keywords:
        try:
            print(f"  -> '{keyword}' ê²€ìƒ‰ ì¤‘...")
            since_date = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
            search_url = f"https://x.com/search?q={keyword}%20since%3A{since_date}&src=typed_query&f=live"
            driver.get(search_url)
            
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, 'article[data-testid="tweet"]'))
            )
            time.sleep(2)

            for _ in range(5):
                tweets = driver.find_elements(By.CSS_SELECTOR, 'article[data-testid="tweet"]')
                for tweet in tweets:
                    try:
                        link_element = tweet.find_element(By.CSS_SELECTOR, 'a[href*="/status/"]')
                        tweet_link = link_element.get_attribute('href')

                        if tweet_link in processed_links:
                            continue
                        
                        text = tweet.find_element(By.CSS_SELECTOR, '[data-testid="tweetText"]').text
                        post_time = tweet.find_element(By.TAG_NAME, 'time').get_attribute('datetime')
                        user_info = tweet.find_element(By.CSS_SELECTOR, '[data-testid="User-Name"]').text.split('\n')
                        username = user_info[0] if user_info else "Unknown"

                        all_results.append({
                            'keyword': keyword,
                            'username': username,
                            'time': post_time,
                            'text': text,
                            'link': tweet_link
                        })
                        processed_links.add(tweet_link)
                    except Exception:
                        continue
                
                driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(2)
            
        except Exception as e:
            print(f"    âŒ '{keyword}' ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            continue
    
    print(f"âœ… [ì„±ê³µ] ì´ {len(all_results)}ê°œ íŠ¸ìœ— ìˆ˜ì§‘ ì™„ë£Œ")
    return sorted(all_results, key=lambda x: x['time'], reverse=True)

# ===== 3. ë©”ì¸ ì‹¤í–‰ ë¡œì§ =====
def main():
    driver = create_stealth_driver()
    if driver:
        naver_keywords = collect_naver_trends(driver)
        google_keywords = collect_google_trends(driver)
        twitter_results = []
        if twitter_cookie_login(driver):
            twitter_results = collect_twitter_data(driver)
        driver.quit()
    else:
        naver_keywords, google_keywords, twitter_results = [], [], []

    dcinside_keywords = collect_dcinside_trends()
    theqoo_keywords = collect_theqoo_trends()

    os.makedirs("reports", exist_ok=True)
    report_path = f"reports/Trend_Report_{{time.strftime('%Y-%m-%d_%H%M%S')}}.txt"
    with open(report_path, "w", encoding="utf-8") as f:
        f.write(f"=== Trend Report ({time.strftime('%Y-%m-%d %H:%M:%S')}) ===\n\n")
        f.write("â–  NAVER\n" + ("\n".join(f"- {k}" for k in naver_keywords) if naver_keywords else "ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨") + "\n\n")
        f.write("â–  Google Trends\n" + ("\n".join(f"- {k}" for k in google_keywords) if google_keywords else "ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨") + "\n\n")
        f.write("â–  DCInside\n" + ("\n".join(f"- {k}" for k in dcinside_keywords) if dcinside_keywords else "ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨") + "\n\n")
        f.write("â–  Theqoo\n" + ("\n".join(f"- {k}" for k in theqoo_keywords) if theqoo_keywords else "ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨") + "\n\n")
        
        f.write("---\n\n")
        f.write(f"=== íŠ¸ìœ„í„° ìŠ¤íƒœí”„ ëª¨ì§‘ ê²€ìƒ‰ ê²°ê³¼ ({time.strftime('%Y-%m-%d %H:%M:%S')}) ===\n")
        f.write(f"ì´ {len(twitter_results)}ê°œ ë°œê²¬\n\n")
        
        if twitter_results:
            for i, result in enumerate(twitter_results, 1):
                f.write(f"[{i}] ê²€ìƒ‰ í‚¤ì›Œë“œ: {result['keyword']}\n")
                f.write(f"ì‘ì„±ì: {result['username']}\n")
                f.write(f"ì‹œê°„: {result['time']}\n")
                f.write(f"ë‚´ìš©: {result['text']}\n")
                f.write(f"ë§í¬: {result['link']}\n")
                f.write("---" * 70 + "\n\n")
        else:
            f.write("ë°ì´í„° ìˆ˜ì§‘ì— ì‹¤íŒ¨í–ˆê±°ë‚˜ ìƒˆë¡œìš´ ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤.\n")

    print(f"\nğŸ“Š ë¦¬í¬íŠ¸ ì €ì¥ ì™„ë£Œ: {report_path}")

if __name__ == "__main__":
    main()
