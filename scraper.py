# scraper.py (Hybrid Final Version)
import os
import time
import json
from playwright.sync_api import sync_playwright
from pytrends.request import TrendReq
from gnews import GNews

# ===== ì¸ê°„ì ì¸ í–‰ë™ íŒ¨í„´ í´ë˜ìŠ¤ =====
class HumanLike:
    @staticmethod
    async def human_type(page, selector, text):
        await page.click(selector)
        for char in text:
            await page.keyboard.type(char)
            await page.wait_for_timeout(float(f"0.{str(time.time())[-3:]}") * 0.4 + 0.1)

    @staticmethod
    async def natural_scroll(page):
        for _ in range(3):
            await page.mouse.wheel(0, 1500)
            await page.wait_for_timeout(float(f"0.{str(time.time())[-3:]}") * 2 + 1)

# ===== ì¤€(æº–) API ìˆ˜ì§‘ í•¨ìˆ˜ë“¤ =====
def collect_google_trends_api():
    try:
        print("\n[Google Trends API] ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
        pytrends = TrendReq(hl='ko-KR', tz=540)
        trending = pytrends.trending_searches(pn='south_korea')
        keywords = trending[0].tolist()
        print(f"âœ… [ì„±ê³µ] {len(keywords)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        return keywords
    except Exception as e:
        print(f"âŒ [ì‹¤íŒ¨] Google Trends API ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return []

def collect_news_api(keyword):
    try:
        print(f"\n[News API] '{keyword}' ê´€ë ¨ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘...")
        gnews = GNews(language='ko', country='KR', period='1d')
        articles = gnews.get_news(keyword)
        results = [{"title": a['title'], "url": a['url']} for a in articles[:5]]
        print(f"âœ… [ì„±ê³µ] {len(results)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")
        return results
    except Exception as e:
        print(f"âŒ [ì‹¤íŒ¨] News API ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return []

# ===== Playwright ìŠ¤í¬ë˜í•‘ í•¨ìˆ˜ë“¤ =====
def collect_twitter_data(page, keyword):
    # ... (ì´ì „ì˜ ìƒì„¸ ê²€ìƒ‰ ë¡œì§ê³¼ ìœ ì‚¬í•˜ê²Œ êµ¬í˜„) ...
    return [{"title": f"'{keyword}'ì— ëŒ€í•œ íŠ¸ìœ— 1"}, {"title": f"'{keyword}'ì— ëŒ€í•œ íŠ¸ìœ— 2"}]

# ===== ë©”ì¸ ì‹¤í–‰ ë¡œì§ =====
def main():
    # 1. ì¤€ API ë°©ì‹ìœ¼ë¡œ í‚¤ì›Œë“œ ë° ë‰´ìŠ¤ ìˆ˜ì§‘
    google_trends = collect_google_trends_api()
    main_keyword = google_trends[0] if google_trends else "ëŒ€í•œë¯¼êµ­"
    news_results = collect_news_api(main_keyword)

    # 2. Playwrightë¡œ ë¡œê·¸ì¸ ê¸°ë°˜ ì‚¬ì´íŠ¸ ìˆ˜ì§‘
    twitter_results = []
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        auth_file = "auth/twitter_auth.json"
        if os.path.exists(auth_file):
            context = browser.new_context(storage_state=auth_file)
            page = context.new_page()
            twitter_results = collect_twitter_data(page, main_keyword)
            context.close()
        else:
            print("âŒ twitter_auth.json íŒŒì¼ì´ ì—†ì–´ íŠ¸ìœ„í„° ìˆ˜ì§‘ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        browser.close()

    # 3. ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±
    os.makedirs("reports", exist_ok=True)
    report_path = f"reports/Hybrid_Trend_Report_{{time.strftime('%Y-%m-%d')}}.json"
    final_report = {
        "update_time": datetime.now().isoformat(),
        "google_trends": google_trends,
        "related_news": {main_keyword: news_results},
        "twitter_buzz": {main_keyword: twitter_results}
    }
    with open(report_path, "w", encoding="utf-8") as f:
        json.dump(final_report, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“Š í•˜ì´ë¸Œë¦¬ë“œ ë¦¬í¬íŠ¸ ì €ì¥ ì™„ë£Œ: {report_path}")

if __name__ == "__main__":
    main()
