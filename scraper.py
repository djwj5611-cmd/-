# scraper.py (Final Stable Version)
import os
import time
import requests
import re
from bs4 import BeautifulSoup
from playwright.sync_api import sync_playwright
from datetime import datetime, timedelta

# ===== ë°ì´í„° ìˆ˜ì§‘ í•¨ìˆ˜ë“¤ =====
def collect_naver_trends(page):
    try:
        print("\n[NAVER] ìˆ˜ì§‘ ì‹œì‘...")
        page.goto("https://signal.bz/", wait_until='domcontentloaded', timeout=30000)
        page.wait_for_selector('div.rank-keyword span.keyword', timeout=15000)
        elements = page.locator('div.rank-keyword span.keyword').all()
        keywords = list(dict.fromkeys([el.inner_text() for el in elements if el.inner_text()]))
        print(f"âœ… [ì„±ê³µ] {len(keywords)}ê°œ ìˆ˜ì§‘")
        return keywords[:10]
    except Exception as e:
        print(f"âŒ [ì‹¤íŒ¨] NAVER: {e}")
        page.screenshot(path="screenshots/naver_error.png")
        return []

def collect_google_trends(page):
    try:
        print("\n[GOOGLE] ìˆ˜ì§‘ ì‹œì‘...")
        page.goto("https://trends.google.com/trends/trendingsearches/daily?geo=KR", wait_until='domcontentloaded', timeout=30000)
        page.wait_for_selector("div.feed-item-header div.title a", timeout=15000)
        elements = page.locator("div.feed-item-header div.title a").all()
        keywords = [el.inner_text().strip() for el in elements if el.inner_text().strip()]
        print(f"âœ… [ì„±ê³µ] {len(keywords)}ê°œ ìˆ˜ì§‘")
        return keywords[:10]
    except Exception as e:
        print(f"âŒ [ì‹¤íŒ¨] GOOGLE: {e}")
        page.screenshot(path="screenshots/google_error.png")
        return []

def collect_dcinside_trends():
    try:
        print("\n[DCINSIDE] ìˆ˜ì§‘ ì‹œì‘...")
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36'}
        res = requests.get("https://gall.dcinside.com/board/lists/?id=hit", headers=headers, timeout=10)
        soup = BeautifulSoup(res.text, "html.parser")
        titles = [re.sub(r'\s*\[\d+\]$', '', el.text.strip()) for el in soup.select("tr.ub-content.us-post .gall_tit a:not(.icon_notice)")]
        print(f"âœ… [ì„±ê³µ] {len(titles)}ê°œ ìˆ˜ì§‘")
        return titles[:10]
    except Exception as e:
        print(f"âŒ [ì‹¤íŒ¨] DCINSIDE: {e}")
        return []

def collect_theqoo_trends():
    try:
        print("\n[THEQOO] ìˆ˜ì§‘ ì‹œì‘...")
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36'}
        res = requests.get("https://theqoo.net/hot", headers=headers, timeout=10)
        soup = BeautifulSoup(res.text, "html.parser")
        titles = [el.text.strip() for row in soup.select("table.theqoo_hot_table tr:not(.notice)") if (el := row.select_one('td.title a'))]
        print(f"âœ… [ì„±ê³µ] {len(titles)}ê°œ ìˆ˜ì§‘")
        return titles[:20]
    except Exception as e:
        print(f"âŒ [ì‹¤íŒ¨] THEQOO: {e}")
        return []

def collect_twitter_data(page):
    print("\n[TWITTER] ì±„ìš© ê³µê³  ê²€ìƒ‰ ì‹œì‘...")
    base_keywords = ["ìŠ¤í…", "ìŠ¤íƒœí”„", "ì§ì›", "íŒ€ì›", "íŒ€", "ë§¤ë‹ˆì €"]
    action_keywords = ["ê³µê³ ", "êµ¬ì¸", "ì±„ìš©", "ëª¨ì§‘"]
    search_keywords = [f"{base} {action}" for base in base_keywords for action in action_keywords]
    all_results = []
    processed_links = set()

    for keyword in search_keywords:
        try:
            print(f"  -> '{keyword}' ê²€ìƒ‰...")
            since_date = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
            search_url = f"https://x.com/search?q={keyword}%20since%3A{since_date}&src=typed_query&f=live"
            page.goto(search_url, wait_until='domcontentloaded', timeout=30000)
            page.wait_for_selector('article[data-testid="tweet"]', timeout=15000)
            
            for _ in range(3):
                page.mouse.wheel(0, 1500)
                time.sleep(1.5)

            tweets = page.locator('article[data-testid="tweet"]').all()
            for tweet in tweets:
                try:
                    link_element = tweet.locator('a[href*="/status/"]').first
                    tweet_link = link_element.get_attribute('href')
                    if tweet_link in processed_links: continue
                    
                    text = tweet.locator('[data-testid="tweetText"]').inner_text()
                    post_time = tweet.locator('time').get_attribute('datetime')
                    username = tweet.locator('[data-testid="User-Name"]').inner_text().split('\n')[0]

                    all_results.append({'keyword': keyword, 'username': username, 'time': post_time, 'text': text, 'link': f"https://x.com{tweet_link}"})
                    processed_links.add(tweet_link)
                except Exception: continue
        except Exception as e:
            print(f"    âŒ '{keyword}' ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            continue
    
    print(f"âœ… [ì„±ê³µ] ì´ {len(all_results)}ê°œ íŠ¸ìœ— ìˆ˜ì§‘")
    return sorted(all_results, key=lambda x: x['time'], reverse=True)

# ===== ë©”ì¸ ì‹¤í–‰ ë¡œì§ =====
def main():
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True, args=['--use-gl=egl'])
        user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36'
        
        # Playwright ê¸°ë°˜ ìˆ˜ì§‘
        page = browser.new_page(user_agent=user_agent)
        naver_keywords = collect_naver_trends(page)
        google_keywords = collect_google_trends(page)
        page.close()

        # íŠ¸ìœ„í„° ìˆ˜ì§‘
        twitter_results = []
        auth_file = "auth/auth_state.json"
        if os.path.exists(auth_file):
            try:
                context = browser.new_context(storage_state=auth_file, user_agent=user_agent)
                page = context.new_page()
                twitter_results = collect_twitter_data(page)
                context.close()
            except Exception as e:
                print(f"âŒ íŠ¸ìœ„í„° ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        else:
            print("âŒ auth.json íŒŒì¼ì´ ì—†ì–´ íŠ¸ìœ„í„° ìˆ˜ì§‘ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        
        browser.close()

    # requests ê¸°ë°˜ ìˆ˜ì§‘
    dcinside_keywords = collect_dcinside_trends()
    theqoo_keywords = collect_theqoo_trends()

    # ë¦¬í¬íŠ¸ ìƒì„±
    os.makedirs("reports", exist_ok=True)
    report_path = f"reports/Trend_Report_{{time.strftime('%Y-%m-%d_%H%M%S')}}.txt"
    with open(report_path, "w", encoding="utf-8") as f:
        f.write(f"=== Trend Report ({time.strftime('%Y-%m-%d %H%M%S')}) ===\n\n")
        f.write("â–  NAVER\n" + ("\n".join(f"- {k}" for k in naver_keywords) if naver_keywords else "ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨") + "\n\n")
        f.write("â–  Google Trends\n" + ("\n".join(f"- {k}" for k in google_keywords) if google_keywords else "ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨") + "\n\n")
        f.write("â–  DCInside\n" + ("\n".join(f"- {k}" for k in dcinside_keywords) if dcinside_keywords else "ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨") + "\n\n")
        f.write("â–  Theqoo\n" + ("\n".join(f"- {k}" for k in theqoo_keywords) if theqoo_keywords else "ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨") + "\n\n")
        f.write("---\n\n")
        f.write(f"=== íŠ¸ìœ„í„° ìŠ¤íƒœí”„ ëª¨ì§‘ ê²€ìƒ‰ ê²°ê³¼ ({time.strftime('%Y-%m-%d %H%M%S')}) ===\n")
        f.write(f"ì´ {len(twitter_results)}ê°œ ë°œê²¬\n\n")
        if twitter_results:
            for i, result in enumerate(twitter_results, 1):
                f.write(f"[{i}] ê²€ìƒ‰ í‚¤ì›Œë“œ: {result['keyword']}\n"
                        f"ì‘ì„±ì: {result['username']}\n"
                        f"ì‹œê°„: {result['time']}\n"
                        f"ë‚´ìš©: {result['text']}\n"
                        f"ë§í¬: {result['link']}\n"
                        f"{'-' * 70}\n\n")
        else:
            f.write("ë°ì´í„° ìˆ˜ì§‘ì— ì‹¤íŒ¨í–ˆê±°ë‚˜ ìƒˆë¡œìš´ ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤.\n")
    print(f"\nğŸ“Š ë¦¬í¬íŠ¸ ì €ì¥ ì™„ë£Œ: {report_path}")

if __name__ == "__main__":
    main()